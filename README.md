# âœ¨ TinyLLMâ€‘RAGâ€‘MOE

![](img/icon.png)

> A lightweight Chinese LLM with Mixtureâ€‘ofâ€‘Experts & DeepSeekâ€‘style attention, and upcoming RAG support.

---

## ğŸ” é¡¹ç›®ç®€ä»‹

**TinyLLMâ€‘RAGâ€‘MOE** æ˜¯ä¸€ä¸ªå°å‹å¤§è¯­è¨€æ¨¡å‹é¡¹ç›®ï¼Œä¸»è¦é¢å‘å­¦ä¹ å¤§æ¨¡å‹æµç¨‹ï¼Œé›†æˆï¼š

- **MoE**ï¼šMixtureâ€‘ofâ€‘Experts å±‚ï¼ˆ4 ä¸ªä¸“å®¶ï¼ŒTopâ€‘1 è·¯ç”±ï¼‰
- **DeepSeekâ€‘V3 Attention**ï¼šä½ç§© KV å‹ç¼©ï¼ˆ4096â†’400ï¼‰ + è§£è€¦æ—‹è½¬ä½ç½®ç¼–ç  RoPE
- **æµç¨‹**ï¼šPTM â†’ SFT\LORA å¾®è°ƒ â†’ MoE â‡¢ ï¼ˆåç»­ RAGï¼‰
- **è½»é‡åŒ–éƒ¨ç½²**ï¼šæ”¯æŒé‡åŒ–ã€Streamlit æ¼”ç¤ºï¼ˆåç»­ä¼šéƒ¨ç½²åˆ°é­”æ­ç¤¾åŒºå’Œ Huggingfaceï¼‰

  > RAG åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­ï¼Œæœ¬äººä¹Ÿæ˜¯åˆå­¦è€…ï¼Œè¾¹å®è·µè¾¹å­¦ä¹ è¾¹åˆ†äº«

## ğŸ“ é¡¹ç›®ç»“æ„

```bash
TinyLLM-RAG-MOE/
â”œâ”€â”€ data/                # åŸå§‹ä¸é¢„å¤„ç†æ•°æ®
â”œâ”€â”€ img/                 # æ¼”ç¤º
â”œâ”€â”€ outputs/             # è®­ç»ƒåŠæ¨ç†è¾“å‡º
â”œâ”€â”€ quantize/            # é‡åŒ–å·¥å…·è„šæœ¬
â”œâ”€â”€ script/              # å¯åŠ¨ä¸éƒ¨ç½²è„šæœ¬
â”œâ”€â”€ tokenizer/           # åˆ†è¯å™¨ä¸è¯è¡¨
â”œâ”€â”€ train/               # è®­ç»ƒä»£ç ä¸æ¨¡å‹æ–‡ä»¶(åŒ…å«Llama2ç‰ˆæœ¬å’ŒMOEç‰ˆæœ¬)
â”‚â€”â€”â€” LoRA.py             #LoRAå¾®è°ƒä»£ç 
â”œâ”€â”€ utils/               # å·¥å…·å‡½æ•°æ¨¡å—
â”œâ”€â”€ finetune.py          # åŸºæœ¬å¾®è°ƒå…¥å£
â”œâ”€â”€ llm_test.py          # åŸºæœ¬æ¨ç†æµ‹è¯•
â”œâ”€â”€ web_demo.py          # Streamlit æ¼”ç¤º
â”œâ”€â”€ moe_test.py          # MoEæµ‹è¯•
â””â”€â”€ requirements.txt     # ä¾èµ–åˆ—è¡¨
```

### demo å±•ç¤º

![](img/demo.png)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒä¾èµ–

```bash
pip install -r requirements.txt
```

- Python â‰¥3.8
- PyTorch â‰¥2.0
- transformers â‰¥4.37
- CUDA â‰¥11.4ï¼ˆå¦‚è®­ç»ƒï¼‰

### ä¸‹è½½æˆ–åŠ è½½æ¨¡å‹

**Hugging Face**:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
model_id = "wdndev/tiny_llm_sft_92m"
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", trust_remote_code=True)
```

**æœ¬åœ°åŠ è½½**ï¼šä¿®æ”¹ `model_id` ä¸ºæœ¬åœ°è·¯å¾„ã€‚

## ğŸ“ è®­ç»ƒæµç¨‹

1. **PTM**: é¢„è®­ç»ƒï¼ˆ`train/ptm_train.py`ï¼‰
2. **SFT**: æŒ‡ä»¤å¾®è°ƒï¼ˆ`train/sft_train.py`ï¼‰
3. **MoE**: æ·»åŠ ä¸“å®¶ç½‘ç»œï¼ˆ`train/LoRA.py` æˆ– `train/moe_train.py`ï¼‰
4. **ï¼ˆåç»­ï¼‰RAG**: æ£€ç´¢å¢å¼ºç”Ÿæˆæ¨¡å—

```bash
python train/sft_train.py  # SFT å¾®è°ƒç¤ºä¾‹
python train/LoRA.py       # LoRA+MoE å¾®è°ƒç¤ºä¾‹
```

## ğŸ¤– æ¨ç†ä¸éƒ¨ç½²

- **æœ¬åœ° Python**: `llm_test.py` äº¤äº’æµ‹è¯•
- **Streamlit Demo**:

  ```bash
  streamlit run web_demo.py
  ```

###### è‡´è°¢:

[wdndev](https://github.com/wdndev/tiny-llm-zh/tree/llama2_torch)ã€[LLama](https://github.com/meta-llama/llama)

## ğŸ”§ æœªæ¥è§„åˆ’

- ğŸŒŸ **RAG**ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œé›†æˆå‘é‡æ£€ç´¢ï¼ˆMilvus/FAISSï¼‰ å¼€å‘ Agent
- ğŸ“ˆ **å¤šå¡åˆ†å¸ƒå¼**ï¼šDeepSpeed & ZeRO åŠ é€Ÿ
- âš™ï¸ **æ›´ä¸°å¯Œè¯„ä¼°**ï¼šåŠ å…¥ QAã€CMMLUã€C-Eval åŸºå‡†æµ‹è¯•

---

Made with â¤ï¸ by **wdndev** | â­ï¸ If you find this useful, please star!
